{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Retail Shelf Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: Click the following button to show or hide the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Show/Hide Code\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Show/Hide Code\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's competitive and evolving __Consumer Packaged Goods (CPG)__ market, shelf management is a very important aspect for CPG manufacturers and retailers. __Automated Retail Shelf Analytics__ is a powerful, accurate, and efficient way for CPG manufacturers and retailers to collect, measure, and analyze what is happening on the physical shelf; It helps them improve operational efficiencies, elevate shopper experience, and maximize revenues. In __Automated Retail Shelf Analytics__, power of __advanced computer vision__ is harnessed and store shelves images are transformed into a comprehensive set of SKU-level metrics and insights which are listed as follows:\n",
    "1. Count of each SKU\n",
    "2. Linear shelf share of each SKU\n",
    "3. List of out of stock SKUs\n",
    "4. Metrics related to the placement of SKUs\n",
    "5. Promos and price tags displayed against the SKUs\n",
    "\n",
    "Once shelf pictures are captured and uploaded to the server, this technology automatically recognizes all SKUs, computes the aforementioned KPIs, and delivers actionable reports back in the hands of the sales reps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/intro_rsa.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To develop an __Intelligent Retail Shelf Analytics__ system which can accept multiple shelf images at once, and generate aforementioned valuable metrics and actionable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the modules of the system are as follows, which are explained in detail in coming sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/System_pipeline_rsa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this project was for one of our CPG clients at __Fractal Analytics__, I cannot present the project details on actual shelf data, so I will be presenting all the details with a public dataset that is available on __Github__ for similar research. The dataset can be downloaded from https://github.com/gulvarol/grocerydataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few details regarding the dataset are as follows:\n",
    "1. It was collected from around 40 groceries, with 4 cameras.\n",
    "2. It consists of products from 10 different cigarette brands.\n",
    "3. It consists of a total of 354 shelf images.\n",
    "4. Out of these 354 shelf images, 300 has been used for training and the rest has been used for testing.\n",
    "5. Annotation looks as shown below. Where the first entry is shelf image name (C1_P04_N1_S3_2.JPG), then the number of products on that shelf (42), then (x,y,w,h) (1024, 1660, 228, 336) corresponding to these bounding boxes, and then brand id of the product (0).\n",
    "\n",
    "![title](images/Annotation_rsa.png)\n",
    "6. It has only bounding box co-ordinates but no class information for all these bounding boxes.\n",
    "7. Co-ordinates are given as (x, y, w, h) where (x, y) is co-ordinate of upper-left corner, and (w,h) is the width and the height of the bounding box.\n",
    "8. All the bounding boxes are labeled as class 0. Therefore, data for classification has to be labeled manually.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/Shelf_1_rsa.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation details and results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Stitching Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few implementation details of this module are as follows:\n",
    "1. It can take one or more images from one or more shelves.\n",
    "2. Images must be taken with some overlap for each shelf.\n",
    "3. Images must be inputted in left to right order.\n",
    "4. It stitches two images at a time and repeats the process.\n",
    "5. Images must be resized such that they have equal height.\n",
    "6. It uses __Sift descriptor__ to stitch images based on matching key-points between the images.\n",
    "7. No. of matched key-points must be greater than 5% of the total no. of pixels in the smaller image, otherwise, we say that there is not sufficient overlap and stitching can't be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section contains the code for image stitching module.\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import errno\n",
    "\n",
    "fileLog = open('myapp.txt','a')\n",
    "\n",
    "# Use the keypoints to stitch the images\n",
    "def get_stitched_image(img1, img2, M):\n",
    "\n",
    "\t# Get width and height of input images\t\n",
    "\tw1,h1 = img1.shape[:2]\n",
    "\tw2,h2 = img2.shape[:2]\n",
    "\n",
    "\t# Get the canvas dimesions\n",
    "\timg1_dims = np.float32([ [0,0], [0,w1], [h1, w1], [h1,0] ]).reshape(-1,1,2)\n",
    "\timg2_dims_temp = np.float32([ [0,0], [0,w2], [h2, w2], [h2,0] ]).reshape(-1,1,2)\n",
    "\n",
    "\n",
    "\t# Get relative perspective of second image\n",
    "\timg2_dims = cv2.perspectiveTransform(img2_dims_temp, M)\n",
    "\t#print(img1_dims,img2_dims)\n",
    "\t# Resulting dimensions\n",
    "\tresult_dims = np.concatenate( (img1_dims, img2_dims), axis = 0)\n",
    "\n",
    "\t# Getting images together\n",
    "\t# Calculate dimensions of match points\n",
    "\t[x_min, y_min] = np.int32(result_dims.min(axis=0).ravel() - 0.5)\n",
    "\t[x_max, y_max] = np.int32(result_dims.max(axis=0).ravel() + 0.5)\n",
    "\t\n",
    "\t# Create output array after affine transformation \n",
    "\ttransform_dist = [-x_min,-y_min]\n",
    "\ttransform_array = np.array([[1, 0, transform_dist[0]], \n",
    "\t\t\t\t\t\t\t\t[0, 1, transform_dist[1]], \n",
    "\t\t\t\t\t\t\t\t[0,0,1]]) \n",
    "\t#print(x_max-x_min, y_max-y_min)\n",
    "\t# Warp images to get the resulting image\n",
    "\tresult_img = cv2.warpPerspective(img2, transform_array.dot(M), \n",
    "\t\t\t\t\t\t\t\t\t(x_max-x_min, y_max-y_min))\n",
    "\tresult_img[transform_dist[1]:w1+transform_dist[1], \n",
    "\t\t\t\ttransform_dist[0]:h1+transform_dist[0]] = img1\n",
    "\n",
    "\t# Return the result\n",
    "\treturn result_img\n",
    "\n",
    "# Find SIFT and return Homography Matrix\n",
    "def get_sift_homography(img1, img2):\n",
    "\n",
    "\t# Initialize SIFT \n",
    "\tsift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "\t# Extract keypoints and descriptors\n",
    "\tk1, d1 = sift.detectAndCompute(img1, None)\n",
    "\tk2, d2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "\t# Bruteforce matcher on the descriptors\n",
    "\tbf = cv2.BFMatcher()\n",
    "\tmatches = bf.knnMatch(d1,d2, k=2)\n",
    "\n",
    "\t# Make sure that the matches are good\n",
    "\tverify_ratio = 0.8 # Source: stackoverflow\n",
    "\tverified_matches = []\n",
    "\tfor m1,m2 in matches:\n",
    "\t\t# Add to array only if it's a good match\n",
    "\t\tif m1.distance < 0.8 * m2.distance:\n",
    "\t\t\tverified_matches.append(m1)\n",
    "\n",
    "\t# Mimnum number of matches\n",
    "\t#min_matches = 1000\n",
    "\tfileLog.write(\"VERIFIED MATCHES: \"+str(len(verified_matches)))\n",
    "\t#if len(verified_matches) > min_matches:\n",
    "\t\t\n",
    "\t# Array to store matching points\n",
    "\timg1_pts = []\n",
    "\timg2_pts = []\n",
    "\n",
    "\t# Add matching points to array\n",
    "\tfor match in verified_matches:\n",
    "\t\timg1_pts.append(k1[match.queryIdx].pt)\n",
    "\t\timg2_pts.append(k2[match.trainIdx].pt)\n",
    "\timg1_pts = np.float32(img1_pts).reshape(-1,1,2)\n",
    "\timg2_pts = np.float32(img2_pts).reshape(-1,1,2)\n",
    "\t\n",
    "\t# Compute homography matrix\n",
    "\tM, mask = cv2.findHomography(img1_pts, img2_pts, cv2.RANSAC, 5.0)\n",
    "\treturn (M,len(verified_matches))\n",
    "\t#else:\n",
    "\t#\tprint('Error: Not enough matches')\n",
    "\t#\texit()\n",
    "\n",
    "# Equalize Histogram of Color Images\n",
    "def equalize_histogram_color(img):\n",
    "\timg_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "\timg_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n",
    "\timg = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n",
    "\treturn img\n",
    "\n",
    "def stitch_two(img1,img2):\n",
    "\timg1 = equalize_histogram_color(img1)\n",
    "\timg2 = equalize_histogram_color(img2)\n",
    "\tM,len_verified_matches =  get_sift_homography(img1, img2)\n",
    "\tstitch_res = get_stitched_image(img2, img1, M)\n",
    "\treturn (stitch_res,len_verified_matches)\n",
    "# Main function definition\n",
    "\n",
    "def one_stitch_iter(filepath, img1, img2_idx, stitch_out = [], stitchIndex = [], size_thresh = 0.8):\n",
    "\t#stitch_out=[]\n",
    "\tlen_verified_matches_lst = []\n",
    "\ttemp_stitch_lst = []\n",
    "\tmin_matches = 1000\n",
    "\t# print(\"inone img2_idx\", img2_idx)\n",
    "\t\n",
    "\tfor i in img2_idx:\n",
    "\t\timg2 = cv2.imread(filepath[i])\n",
    "\t\ttemp_stitch, len_verified_matches = stitch_two(img1, img2)\n",
    "\t\t#print(temp_stitch.shape,img1.shape,img2.shape)\n",
    "\t\tif(temp_stitch.shape[1] > size_thresh*(img1.shape[1] + img2.shape[1]) and len_verified_matches > min_matches):\n",
    "\t\t\tlen_verified_matches_lst.append(len_verified_matches)\n",
    "\t\telse:\n",
    "\t\t\tlen_verified_matches_lst.append(-1)\n",
    "\t\ttemp_stitch_lst.append(temp_stitch)\n",
    "\n",
    "\tmax_len_verified_matches, max_len_verified_matches_idx = np.max(len_verified_matches_lst), np.argmax(len_verified_matches_lst)\n",
    "\t\n",
    "\tif(max_len_verified_matches > -1):\n",
    "\t\tadd_stitch_res = True\n",
    "\t\timg1 = temp_stitch_lst[max_len_verified_matches_idx]\n",
    "\t\tstitchIndex.append(img2_idx[max_len_verified_matches_idx])\n",
    "\t\timg2_idx.remove(img2_idx[max_len_verified_matches_idx])\n",
    "\t\tstitch_out.append(temp_stitch_lst[max_len_verified_matches_idx])\n",
    "\t\n",
    "\t\tif(len(img2_idx) != 0):\n",
    "\t\t\tstitch_out, img2_idx, stitchIndex = one_stitch_iter(filepath, img1, img2_idx, stitch_out, stitchIndex)\n",
    "\n",
    "\treturn (stitch_out, img2_idx, stitchIndex)\n",
    "\n",
    "def main(filepath):\n",
    "\tpath = os.path.join('/data1/naquib.alam/images', filepath)\n",
    "\n",
    "\t#Get number of input images\n",
    "\tfilelist = []\n",
    "\n",
    "\tfor f in os.listdir(path):\n",
    "\t\tfilelist.append(os.path.join(path, f))\n",
    "\tn_images = len(filelist)\n",
    "\tfilelist = sorted(filelist)\n",
    "\t# print(filelist)\n",
    "\t# print(filelist)\n",
    "\n",
    "\t# img1 = cv2.imread(filepath[0])\n",
    "\tres_images = []\n",
    "\tres_img_names = []\n",
    "\tstitch_possible = True\n",
    "\tcnt = 0\n",
    "\t# Get input set of images\n",
    "\timg2_idx = list(range(0, n_images))\n",
    "\tstitch_res = []\n",
    "\t# while True:\n",
    "\tstitch_indices = []\n",
    "\twhile len(img2_idx) > 1:\n",
    "\t\t# print(\"ID:\", img2_idx)\n",
    "\t\t\n",
    "\t\timg1 = cv2.imread(filelist[img2_idx[0]])\n",
    "\t\ttemp = [img2_idx[0]]\n",
    "\t\timg2_idx.pop(0)\n",
    "\t\t# print(\"IDX:\", img2_idx)\n",
    "\t\tstitch_out, img2_idx, stitch_index_temp = one_stitch_iter(filelist, img1, img2_idx, [], temp)\n",
    "\t\t# print(\"stitch_index_temp: \",stitch_index_temp)\n",
    "\t\tif(len(stitch_out) > 0):\n",
    "\t\t\tstitch_res.append(stitch_out[-1])\n",
    "\t\telse:\n",
    "\t\t\tstitch_res.append(img1)\n",
    "\n",
    "\t\tstitch_indices.append(stitch_index_temp)\n",
    "\t\tfileLog.write(\"LEN:\"+str(len(stitch_res)))\n",
    "\t\t#print(img2_idx)\n",
    "\n",
    "\tif(len(img2_idx) == 1):\n",
    "\t\tstitch_res.append(cv2.imread(filelist[img2_idx[0]]))\n",
    "\t\tstitch_indices.append(img2_idx)\n",
    "\t\t\n",
    "\tfileLog.write(\"LEN FINAL:\"+str(len(stitch_res)))\n",
    "\n",
    "\ttry:\n",
    "\t\tos.mkdir(os.path.join('/data1/naquib.alam/static/temp', filepath))\n",
    "\texcept OSError as exc:\n",
    "\t\ttry:\n",
    "\t\t\tos.mkdir('/data1/naquib.alam/static/temp')\n",
    "\t\t\tos.mkdir(os.path.join('/data1/naquib.alam/static/temp', filepath))\n",
    "\t\texcept OSError as e:\n",
    "\t\t\tif e.errno != errno.EEXIST:\n",
    "\t\t\t\traise\n",
    "\t\t\tpass\n",
    "\n",
    "\t\tif exc.errno != errno.EEXIST:\n",
    "\t\t\traise\n",
    "\t\tpass\n",
    "\n",
    "\n",
    "\t# Save the results\n",
    "\tfor i in range(len(stitch_res)):\n",
    "\t\tres_image_name = '/data1/naquib.alam/static/temp/' + filepath + '/result_' + str(i+1) + '.jpg'\n",
    "\t\t# res_image_nameC = 'static/temp/resultC_'+str(i)+'.JPG'\n",
    "\t\t# if stitch_res[i].shape[0] > 1200:\n",
    "\t\t\t# rows = 1200\n",
    "\t\t\t# cols = int(1200*stitch_res[i].shape[1]/stitch_res[i].shape[0])\n",
    "\t\t# else:\n",
    "\t\ttry:\n",
    "\t\t\tos.mkdir(os.path.join(os.path.join('/data1/naquib.alam/static/temp', filepath), \"result_\"+str(i+1)))\n",
    "\t\texcept OSError as exc:\n",
    "\t\t\tif exc.errno != errno.EEXIST:\n",
    "\t\t\t\traise\n",
    "\t\t\tpass\n",
    "\t\tfor index in stitch_indices[i]:\n",
    "\t\t\tpath = os.path.join(os.path.join('/data1/naquib.alam/static/temp', filepath), \"result_\"+str(i+1))\n",
    "\t\t\tcv2.imwrite(os.path.join(path, str(index)+\".jpg\"), cv2.imread(filelist[index]))\n",
    "\t\trows = stitch_res[i].shape[0]\n",
    "\t\tcols = stitch_res[i].shape[1]\n",
    "\n",
    "\t\tcv2.imwrite(res_image_name, stitch_res[i])\n",
    "\t\tfileLog.close()\n",
    "\t\t# cv2.imwrite(res_image_nameC, cv2.resize(stitch_res[i], (rows, cols)))\n",
    "\n",
    "\t# Save the inputs \n",
    "\t# for f in filelist:\n",
    "\t# \tres_image_nameC = 'static/images/'\n",
    "\t# \timg = cv2.imread(f);\n",
    "\t# \tif img.shape[0] > 1200:\n",
    "\t# \t\trows = 1200\n",
    "\t# \t\tcols = int(1200*img.shape[1]/img.shape[0])\n",
    "\t# \telse:\n",
    "\t# \t\trows = img.shape[0]\n",
    "\t# \t\tcols = img.shape[1]\n",
    "\n",
    "\t# \tcv2.imwrite(res_image_nameC + f[30:], cv2.resize(img, (rows, cols)))\n",
    "\n",
    "\t# for file in os.path.join(\"/data1/naquib.alam/images\", filepath):\n",
    "\t# \tif not os.path.isfile(os.path.join(os.path.join(\"/data1/naquib.alam/images\", filepath), file)):\n",
    "\t# \t\tcontinue\n",
    "\t# \tos.remove(os.path.join(os.path.join(\"/data1/naquib.alam/images\", filepath), file))\n",
    "\t# os.remove(os.path.join(\"/data1/naquib.alam/images\", filepath))\n",
    "\n",
    "\treturn stitch_indices\n",
    "\n",
    "# Call main function\n",
    "if __name__=='__main__':\n",
    "\t# print(sys.argv[1])\n",
    "\tmain(sys.argv[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/Stitch_input_rsa.png)  \n",
    "  \n",
    "__Note:__ Red rectangles here represent the overlapping region between adjacent images to be stitched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/Stitch_output_rsa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few implementation details of this module are as follows:\n",
    "1. Object detection was used just to draw bounding boxes (localization) but not for classification.\n",
    "2. Each bounding box was classified as one class since we didn't have any class information.\n",
    "3. For object detection, __YOLO__ in __Keras__ and __Google API__ in __TensorFlow__ have been used.\n",
    "4. __Google Object Detection API__ was found to give better results.\n",
    "5. In Google API, different architectures have been tried but __Faster_rcnn_resnet50_coco__ architecture was found to perform better.\n",
    "6. Pre-trained weights were fine-tuned with this dataset. For this purpose, __config__ and __labelmap.txt__ files were modified as per our dataset. Labelmap.txt file contains class information and config file contains all the information related to image size; architecture; training; and paths for train.record, val.record and labelmap.txt.\n",
    "7. Finetuned model is exported as a Tensorflow graph proto (.pb format) file which is loaded for inference.\n",
    "8. At inference time, this module outputs 4 tensors:\n",
    "    1. __num_detections__\n",
    "    2. __detection_scores__\n",
    "    3. __detection_boxes__\n",
    "    4. __detection_classes__\n",
    "9. All boxes with a score of less than 0.3 were ignored.\n",
    "10. Overlapping boxes were removed using __Non Maximum Suppression (NMS)__.\n",
    "11. An __F1-score__ of __0.798__ was achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section contains code for object detection module.\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "df = pd.read_csv('/data1/naquib.alam/data/test_labels.csv')\n",
    "\n",
    "with tf.gfile.FastGFile('/data1/naquib.alam/model/frozen_inference_graph.pb','rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.graph.as_default()\n",
    "tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "for name in df.filename.unique():\n",
    "    #name = df.filename[0]\n",
    "    # Read and preprocess an image.\n",
    "    img = Image.open('/data1/naquib.alam/CIGDataset/ShelfImages/'+name)\n",
    "    inp = np.array(img)\n",
    "    inp = inp[:, :, ::-1].copy()\n",
    "    rows = inp.shape[0]\n",
    "    cols = inp.shape[1]\n",
    "    out = sess.run([sess.graph.get_tensor_by_name('num_detections:0'),\n",
    "                sess.graph.get_tensor_by_name('detection_scores:0'),\n",
    "                sess.graph.get_tensor_by_name('detection_boxes:0'),\n",
    "                sess.graph.get_tensor_by_name('detection_classes:0')],\n",
    "               feed_dict={'image_tensor:0': inp.reshape(1, inp.shape[0], inp.shape[1], 3)})\n",
    "    num_detections = int(out[0][0])\n",
    "    \n",
    "    for i in range(num_detections):\n",
    "\tclassId = int(out[3][0][i])\n",
    "    \tscore = float(out[1][0][i])\n",
    "    \tbbox = [float(v) for v in out[2][0][i]]\n",
    "    \tif score > 0.3:\n",
    "      \t   x = bbox[1] * cols\n",
    "           y = bbox[0] * rows\n",
    "           right = bbox[3] * cols\n",
    "           bottom = bbox[2] * rows\n",
    "           cv.rectangle(inp, (int(x), int(y)), (int(right), int(bottom)), (255,0,0), thickness=2)    \n",
    "    \n",
    "\n",
    "    print('Saving file name: ',name)\n",
    "    result = Image.fromarray(inp)\n",
    "    result.save('/data1/naquib.alam/CIGDataset/Results/'+name)\n",
    "    print('Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Classification Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few implementation details of this module are as follows:\n",
    "1. Since all the bounding boxes were annotated with class 0, we had to label the data for classification manually. For this purpose, detected objects from the training data were selected and then further refined to make a better dataset for classification module.\n",
    "2. Total 6550 images were split (stratified) as follows:\n",
    "    1. Training data: 85 %\n",
    "    2. Validation data: 10 %\n",
    "    3. Test data: 5%\n",
    "3. All images were divided into __11 classes__ including one class as __Other__ which contains all those classes which had very few images.\n",
    "\n",
    "4. Different __pre-trained__ architectures were tried but __VGG16__ was found to work better.\n",
    "5. Added __FC layers__ and __last 10 layers__ of the network were fine-tuned.\n",
    "6. Few things which improved the accuracy are as follows:\n",
    "    1. __Data augmentation__\n",
    "    2. __Discriminative fine-tuning__\n",
    "    3. __Gradual unfreezing__\n",
    "    4. __Stochastic Gradient Descent With Restarts__\n",
    "    5. __Adam optimizer__\n",
    "7. An __F1-score__ of __0.976__ was achieved.\n",
    "8. Overall score of both detection and classification was __0.778__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains code for classification module.\n",
    "\n",
    "import os, sys, math, shutil, PIL, json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "import keras \n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from PIL import Image\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "def train_test_img_split(base_dir,dosplit=False):\n",
    "    if dosplit is False:\n",
    "        return\n",
    "    #print(\"returned_2\")\n",
    "    folders=os.listdir(base_dir)\n",
    "    folders=sorted(np.array(folders).astype(int))\n",
    "    folders=np.array(folders).astype(str)\n",
    "    #print(folders)\n",
    "    train_img=[]\n",
    "    val_img=[]\n",
    "    test_img=[]\n",
    "    train_test_img_path=[]\n",
    "    for x in folders:\n",
    "        class_files=os.listdir(os.path.join(base_dir,x))\n",
    "        len_class_files=len(class_files)\n",
    "        #file_sel_idx=np.random.shuffle(np.arange(len_class_files)\n",
    "        end_train_idx=int(len_class_files*0.85)\n",
    "        end_val_idx=int(len_class_files*0.95)\n",
    "        #train_files=class_files[:end_train_idx]\n",
    "        #test_files=class_files[end_train_idx:]\n",
    "        train_files=class_files[:end_train_idx]\n",
    "        val_files=class_files[end_train_idx:end_val_idx]\n",
    "        test_files=class_files[end_val_idx:]\n",
    "        train_img.append(train_files)\n",
    "        val_img.append(val_files)\n",
    "        test_img.append(test_files)\n",
    "        train_test_img_path.append(os.path.join(base_dir,x))\n",
    "    return(train_test_img_path,train_img,val_img,test_img,folders)\n",
    "    \n",
    "def create_train_val_test_dir(train_test_img_src_path,base_dir_split,train_img,val_img,test_img,create_dir=False):\n",
    "    if create_dir is False:\n",
    "        return\n",
    "    #print(\"Returned_2\")\n",
    "    dirs=[\"train\",\"val\",\"test\"]\n",
    "    for i,x in enumerate(dirs):\n",
    "        if (os.path.exists(os.path.join(base_dir_split,x))) is False:\n",
    "            os.mkdir(os.path.join(base_dir_split,x))\n",
    "        if i==0:\n",
    "            for j,y in enumerate(train_img):\n",
    "                if (os.path.exists(os.path.join(base_dir_split,x,str(j)))) is False:\n",
    "                    os.mkdir(os.path.join(base_dir_split,x,str(j)))\n",
    "                for z in y:\n",
    "                    shutil.copy(os.path.join(train_test_img_src_path[j],z),os.path.join(base_dir_split,x,str(j)))\n",
    "        elif i==1:\n",
    "            for j,y in enumerate(val_img):\n",
    "                if (os.path.exists(os.path.join(base_dir_split,x,str(j)))) is False:\n",
    "                    os.mkdir(os.path.join(base_dir_split,x,str(j)))\n",
    "                for z in y:\n",
    "                    shutil.copy(os.path.join(train_test_img_src_path[j],z),os.path.join(base_dir_split,x,str(j)))\n",
    "        else:\n",
    "            for j,y in enumerate(test_img):\n",
    "                if (os.path.exists(os.path.join(base_dir_split,x,str(j)))) is False:\n",
    "                    os.mkdir(os.path.join(base_dir_split,x,str(j)))\n",
    "                for z in y:\n",
    "                    shutil.copy(os.path.join(train_test_img_src_path[j],z),os.path.join(base_dir_split,x,str(j)))\n",
    "\n",
    "def get_number_images(base_dir):\n",
    "    files=[\"train\",\"valid\",\"test\"]\n",
    "    n_images=[0,0,0]\n",
    "    for i,x in enumerate(files):\n",
    "        folders=os.listdir(os.path.join(base_dir,x))\n",
    "        for y in folders:\n",
    "            class_image=os.listdir(os.path.join(base_dir,x,y))\n",
    "            n_images[i]+=len(class_image)\n",
    "    return n_images      \n",
    "       \n",
    "def get_class_weights(y, smooth_factor=0):\n",
    "    \"\"\"\n",
    "    Returns the weights for each class based on the frequencies of the samples\n",
    "    :param smooth_factor: factor that smooths extremely uneven weights\n",
    "    :param y: list of true labels (the labels must be hashable)\n",
    "    :return: dictionary with the weight for each class\n",
    "    \"\"\"\n",
    "    counter = Counter(y)\n",
    "    print(\"COUNTER: \",counter)\n",
    "    if smooth_factor > 0:\n",
    "        p = max(counter.values()) * smooth_factor\n",
    "        for k in counter.keys():\n",
    "            counter[k] += p\n",
    "\n",
    "    majority = max(counter.values())\n",
    "\n",
    "    return {cls: float(majority / count) for cls, count in counter.items()}\n",
    "\n",
    "def fine_tune_dense(base_dir_split,n_images,do_train=False):\n",
    "    if do_train is False:\n",
    "        return\n",
    "    summ_file=open('model_summary.txt','w')\n",
    "    train_dir=os.path.join(base_dir_split,\"train\")\n",
    "    val_dir=os.path.join(base_dir_split,\"valid\")\n",
    "    #test_dir=os.path.join(base_dir_split,\"test\")\n",
    "    img_width, img_height = 224,224\n",
    "    n_class=16\n",
    "    batch_size=32\n",
    "    epochs=20\n",
    "    learning_rate=0.001\n",
    "    decay_rate=1/epochs\n",
    "    nb_train_samples=n_images[0]\n",
    "    nb_validation_samples=n_images[1]\n",
    "    step_per_epoch=nb_train_samples/batch_size\n",
    "    validation_step= nb_validation_samples/batch_size\n",
    "    model = applications.VGG16(weights='imagenet', include_top=False,input_shape = (224,224,3))\n",
    "    print('Model loaded.')\n",
    "    #return model\n",
    "    \n",
    "    top_model = Sequential()\n",
    "    for layer in model.layers:\n",
    "        top_model.add(layer)\n",
    "    top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "    #return model\n",
    "    top_model.add(Dense(256, activation='relu',kernel_initializer='he_normal',bias_initializer=keras.initializers.Constant(value=0.1)))\n",
    "    top_model.add(Dropout(0.2))\n",
    "    top_model.add(Dense(n_class, activation='softmax',kernel_initializer='he_normal',bias_initializer=keras.initializers.Constant(value=0.1)))\n",
    "    #model.add(top_model)\n",
    "    print('Top dense layer added')\n",
    "    #return top_model\n",
    "    for layer in top_model.layers[:19]:\n",
    "        layer.trainable = False\n",
    "    sgd=optimizers.SGD(lr=learning_rate, momentum=0.9, decay=decay_rate)\n",
    "    top_model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    # prepare data augmentation configuration\n",
    "    train_datagen = ImageDataGenerator(rotation_range=15,width_shift_range=0.1,height_shift_range=0.1,zoom_range=0.1,rescale=1. / 255)\n",
    "    val_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,target_size=(img_height, img_width),batch_size=batch_size,class_mode='categorical')\n",
    "\n",
    "    validation_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,target_size=(img_height, img_width),batch_size=batch_size, class_mode='categorical')\n",
    "    #print(\"Train_Class_indices:\",train_generator.class_indices)\n",
    "    #print(\"Val_Class_Indices:\",validation_generator.class_indices)\n",
    "    class_weight=get_class_weights(train_generator.classes,0.1)\n",
    "    #print(\"CLASS_WEIGHTS: \",class_weight)\n",
    "    checkpoint_path='CIGDataset/SplitData/models/KerasTest/VGG16_Adam_30_0.hdf5'\n",
    "    checkpoint= ModelCheckpoint(checkpoint_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callback_list=[checkpoint]  \n",
    "    hist=top_model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=step_per_epoch,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_step,\n",
    "    class_weight=class_weight,\n",
    "    callbacks= callback_list)\n",
    "    \n",
    "    model_json=top_model.to_json()\n",
    "    with open(\"CIGDataset/SplitData/models/KerasTest/VGG16_Adam_30_0.json\",\"w\") as json_file:\n",
    "        json.dump(model_json,json_file)\n",
    "    top_model.save_weights(\"CIGDataset/SplitData/models/KerasTest/VGG16_Adam_30_0.h5\") \n",
    "    with open('CIGDataset/SplitData/models/KerasTest/VGG16_Hist_Adam_30_0.json',\"w\") as json_file:\n",
    "        json.dump(hist.history,json_file)\n",
    "    print(top_model.summary(),file=summ_file)\n",
    "    summ_file.close()\n",
    "    return top_model\n",
    "\n",
    "def do_predict(model_json,weights,base_dir_split,n_images,dopredict=False):\n",
    "    if dopredict is False:\n",
    "        return\n",
    "    test_dir=os.path.join(base_dir_split,\"test\")\n",
    "    nb_test_samples=n_images[2]\n",
    "    #nb_test_samples=7724\n",
    "    img_height,img_width=224,224\n",
    "    batch_size=32\n",
    "    arch_file_json=open(model_json,'r')\n",
    "    model_json=arch_file_json.load()\n",
    "    arch_file_json.close()\n",
    "    loaded_model=model_from_json(model_json)\n",
    "    loaded_model.load_weights(weights)\n",
    "    test_datagen=ImageDataGenerator(rescale=1. / 255)\n",
    "    test_generator=test_datagen.flow_from_directory(test_dir,target_size=(img_height,img_width),batch_size=batch_size,class_mode='categorical',shuffle=False)\n",
    "    print(\"test class indices: \",test_generator.class_indices)\n",
    "    #print(\"test classes: \",test_generator.classes)\n",
    "    pred_label=loaded_model.predict_generator(test_generator,int(math.ceil(nb_test_samples/batch_size))+1)\n",
    "    print(\"Shape: \",pred_label.shape)\n",
    "    y_true=test_generator.classes\n",
    "    y_pred=np.argmax(pred_label,1)\n",
    "    print(confusion_matrix(y_true,y_pred))\n",
    "    #print(np.max(pred_label,1),np.argmax(pred_label,1))\n",
    "    return pred_label\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    base_dir_whole= r\"/data1/naquib.alam/CIGDataset/CroppedImages\"\n",
    "    base_dir_split= r\"/data1/naquib.alam/CIGDataset/SplitData\"\n",
    "    #print(base_dir_whole,base_dir_split)\n",
    "    #x=\"/data1/naquib.alam/ShelfMonitoring/ObjectDetection/CiggarateDataset/grocerydataset/CroppedImageCategoryRefined/WholeData\"\n",
    "    #print(\"Trax\",type(base_dir_whole),x==base_dir_whole)\n",
    "    #train_test_img_src_path,train_img,val_img,test_img,folders = train_test_img_split(base_dir_whole)\n",
    "    #print(type(train_test_img_src_path),type(train_img),type(val_img),type(test_img))\n",
    "    #create_train_val_test_dir(train_test_img_src_path,base_dir_split,train_img,val_img,test_img)\n",
    "    n_images = get_number_images(base_dir_split)\n",
    "    print(n_images)\n",
    "    model = fine_tune_dense(base_dir_split,n_images,True)\n",
    "    #pred_label=do_predict(\"model_30_32_LIT.json\",\"weights_30_32_LIT.h5\",base_dir_split,n_images,True)\n",
    "    #print(np.argmax(pred_label,1))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/Object_detection_1_rsa.png)\n",
    "  \n",
    "1. Bounding boxes with different colors represent different brands of cigarettes classified.\n",
    "2. Red dots represent the misclassified objects.\n",
    "3. Yellow dots represent boxes which were not detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Calculation Module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few implementation details of this module are as follows:\n",
    "1. Outputs from previous modules were used to calculate all the relevant metrics.\n",
    "2. Number of racks on a shelf were found by sorting the x co-ordinates and then finding the transition points.\n",
    "3. Length (width) of each SKU were found by using the predicted width (w) of a bounding box.\n",
    "4. Length of each racks were also found in order to calculate the linear shelf share.\n",
    "5. All these informations were used to calculate aforementioned metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell contains the code for metrics calculation and promo detection module.\n",
    "\n",
    "import os \n",
    "import re\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import style\n",
    "style.use(\"ggplot\")\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "import time, datetime, errno\n",
    "from sklearn.cluster import KMeans\n",
    "import pytesseract\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "class GetOutOfLoop(Exception):\n",
    "    pass\n",
    "\n",
    "class ShelfShare(object):\n",
    "    \n",
    "    graph_def = None\n",
    "    loaded_model = None\n",
    "    noOfPromo = None\n",
    "    \n",
    "    def __init__(self, path = '/data1/naquib.alam/model/frozen_inference_graph.pb',\n",
    "                 model = \"/data1/naquib.alam/model_31_32_2_sgd.json\",\n",
    "                 weights = \"/data1/naquib.alam/weights_31_32_2_sgd.h5\"):\n",
    "    # def trial(self, path = '/data1/naquib.alam/model/frozen_inference_graph.pb',\n",
    "    #              model = \"/data1/naquib.alam/model_31_32_2_sgd.json\",\n",
    "    #              weights = \"/data1/naquib.alam/weights_31_32_2_sgd.h5\"):\n",
    "        if self.graph_def == None:\n",
    "            self._graph_load(path)\n",
    "        if self.loaded_model == None:\n",
    "            self._load_classifier(model, weights)\n",
    "        if self.noOfPromo == None:\n",
    "            self.noOfPromo = []\n",
    "        \n",
    "    def _graph_load(self, path):\n",
    "        graph_path = path\n",
    "        with tf.gfile.FastGFile(graph_path,'rb') as f:\n",
    "            self.graph_def = tf.GraphDef()\n",
    "            self.graph_def.ParseFromString(f.read())\n",
    "            # print('Detection module loaded...')\n",
    "    \n",
    "    def _load_classifier(self, model, weights):\n",
    "        model_json = model\n",
    "        arch_file_json = open(model_json,'r')\n",
    "        model_json = json.load(arch_file_json)\n",
    "        arch_file_json.close()\n",
    "        self.loaded_model = model_from_json(model_json)\n",
    "        self.loaded_model.load_weights(weights)\n",
    "        # print('Classifier loaded...')\n",
    "        \n",
    "    def _image_load(self, path = '/data1/naquib.alam/CIGDataset/ShelfImages/C4_P06_N1_S4_1.JPG'):\n",
    "        image_path = path\n",
    "        img = Image.open(image_path)\n",
    "        self.ing = np.array(img)\n",
    "        self.ing = self.ing[:, :, ::-1].copy()\n",
    "        # print('Image uploaded for processing...')\n",
    "    \n",
    "    def _find_object(self):\n",
    "        \n",
    "        self.imgCropTest = self.ing\n",
    "        self.ing = cv.resize(self.ing,(600,600))\n",
    "                \n",
    "        self._out = self._sess.run([self._sess.graph.get_tensor_by_name('num_detections:0'),\n",
    "            self._sess.graph.get_tensor_by_name('detection_scores:0'),\n",
    "            self._sess.graph.get_tensor_by_name('detection_boxes:0'),\n",
    "            self._sess.graph.get_tensor_by_name('detection_classes:0')],\n",
    "           feed_dict={'image_tensor:0': self.ing.reshape(1, self.ing.shape[0], self.ing.shape[1], 3)})\n",
    "        # print(\"Bounding boxes calculated...\")\n",
    "        self._sess.close()\n",
    "        return self._out\n",
    "    \n",
    "    def _process_boxes(self, inputfolder):\n",
    "        num_detections = int(self._out[0][0])\n",
    "        #bound_box = self._out[2][0]\n",
    "        bound_box = self._remove_overlap(self._out[2][0])\n",
    "        bound_box = self._remove_overlap(bound_box, True)\n",
    "        self._out[2][0] = bound_box\n",
    "        #num_detections = bound_box[(bound_box[:,0] != 0) & (bound_box[:,1] != 0) & (bound_box[:,2] != 0) & (bound_box[:,3] != 0),:].shape[0]\n",
    "        Y = np.zeros([num_detections,2])\n",
    "        #Y[:,0] = bound_box[np.any([bound_box[:,0]!=0, bound_box[:,1]!=0],axis=0),0]\n",
    "        #Y[:,1] = bound_box[np.any([bound_box[:,0]!=0, bound_box[:,1]!=0],axis=0),2]\n",
    "        Y = bound_box[0:num_detections,:]\n",
    "\n",
    "        rack_info = self._find_racks(Y)\n",
    "        self._draw_boxes(rack_info)\n",
    "        results = self._classify_boxes(bound_box, rack_info)\n",
    "        self._write_results(results, rack_info[1], inputfolder)\n",
    "        return results\n",
    "        \n",
    "    def _remove_overlap(self, bound_box, flag=False):\n",
    "        if not flag:\n",
    "            indexS = np.argsort(bound_box[:,1])\n",
    "        else:\n",
    "            indexS = np.argsort(bound_box[:,3])\n",
    "        indexSR = np.argsort(indexS)\n",
    "        bound_box = bound_box[indexS,:]\n",
    "        for i in range(2, bound_box.shape[0]):\n",
    "            if (bound_box[i-1,0] == 0) & (bound_box[i-1,1] == 0) & (bound_box[i-1,2] == 0) & (bound_box[i-1,3] == 0):\n",
    "                continue\n",
    "            area1 = 1000000*(bound_box[i,3]-bound_box[i,1])*(bound_box[i,2]-bound_box[i,0])\n",
    "            area2 = 1000000*(bound_box[i-1,3]-bound_box[i-1,1])*(bound_box[i-1,2]-bound_box[i-1,0])\n",
    "            overlap_area = 1000000*(np.minimum(bound_box[i,3], bound_box[i-1,3])-np.maximum(bound_box[i,1], bound_box[i-1,1]))*(np.minimum(bound_box[i,2], bound_box[i-1,2])-np.maximum(bound_box[i,0], bound_box[i-1,0]))\n",
    "\n",
    "            if overlap_area >= 0.8*np.minimum(area2,area1):\n",
    "                if area2 > area1:\n",
    "                    bound_box[i,:] = 0\n",
    "                else:\n",
    "                    bound_box[i-1,:] = 0\n",
    "                \n",
    "        return bound_box[indexSR,:]\n",
    "\n",
    "    def _find_racks(self, Y):\n",
    "        trial = Y[:,0]\n",
    "        ind = np.argsort(trial)\n",
    "        trial = np.sort(trial)\n",
    "        diff = trial[1:]-trial[:-1]\n",
    "        # diff2 = diff[1:]-diff[:-1]\n",
    "    \n",
    "        labInd = np.zeros(len(trial))\n",
    "        for i in range(len(diff)):\n",
    "            if diff[i] > np.max(diff)*0.3:\n",
    "                labInd[i+1] = labInd[i]+1\n",
    "            else:\n",
    "                labInd[i+1] = labInd[i]\n",
    "    \n",
    "        col = [(255,0,0),(0,255,0),(0,0,255)]\n",
    "        for i in range(3,len(np.unique(labInd))):\n",
    "            col.append((255,random.randint(0,256),random.randint(0,256)))    \n",
    "        # print('Rack information calculated...')\n",
    "        return ind, labInd, col\n",
    "    \n",
    "    def _draw_boxes(self, rack_info):\n",
    "        num_detections = int(self._out[0][0])\n",
    "        rowsCT = self.imgCropTest.shape[0]\n",
    "        colsCT = self.imgCropTest.shape[1]\n",
    "        self.ing = cv.resize(self.ing, (colsCT,rowsCT))\n",
    "\n",
    "        col = rack_info[2]\n",
    "        labInd = rack_info[1]\n",
    "        ind = rack_info[0]\n",
    "\n",
    "        if (rowsCT >= 1500) or (colsCT >= 1500):\n",
    "            thickness = 10\n",
    "        elif (rowsCT >= 500) or (colsCT >= 500):\n",
    "            thickness = 4 + (colsCT/1000 - 0.5)*6\n",
    "        else:\n",
    "            thickness = 4\n",
    "                \n",
    "        for i in range(num_detections):\n",
    "            index = np.where(ind == i)\n",
    "            score = float(self._out[1][0][i])\n",
    "            bbox = [float(v) for v in self._out[2][0][i]]\n",
    "            if (bbox[0] == 0) & (bbox[1] == 0) & (bbox[2] == 0) & (bbox[3] == 0):\n",
    "                continue \n",
    "            if score > 0.3:\n",
    "                x = bbox[1] * colsCT\n",
    "                y = bbox[0] * rowsCT\n",
    "                right = bbox[3] * colsCT\n",
    "                bottom = bbox[2] * rowsCT\n",
    "                cv.rectangle(self.ing, (int(x), int(y)), (int(right), int(bottom)), col[int(labInd[index[0][0]])], thickness=thickness)\n",
    "    \n",
    "        #cv.imshow('TensorFlow MobileNet-SSD', self.ing)\n",
    "\n",
    "    def _draw_promo_boundary(self):\n",
    "        # print(\"Running Contour Detection...\")\n",
    "        imgray = cv.cvtColor(self.ing_promo, cv.COLOR_BGR2GRAY)\n",
    "        ret, thresh = cv.threshold(imgray, 127, 255, 0)\n",
    "        image, contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        # print(\"Total number of contours:\", len(contours))\n",
    "        area_contours=[]\n",
    "\n",
    "        for cnt in contours:\n",
    "            area_contours.append(cv.contourArea(cnt))\n",
    "        area_contours = np.array(area_contours)\n",
    "        top_area_args = np.argsort(-area_contours)[:30]\n",
    "        top_area_contours = area_contours[top_area_args]\n",
    "        rect_xywh_txt = []\n",
    "\n",
    "        for idx in top_area_args:\n",
    "            cnt = contours[idx]\n",
    "            x, y, w, h = cv.boundingRect(cnt)\n",
    "            im_ocr = self.ing_promo[y:y+h, x:x+w]\n",
    "            im_ocr_gray = cv.cvtColor(im_ocr, cv.COLOR_BGR2GRAY)\n",
    "            _, im_ocr_thresh = cv.threshold(im_ocr_gray, 127, 255, 0)\n",
    "\n",
    "            text = pytesseract.image_to_string(Image.fromarray(im_ocr_thresh))\n",
    "            print(len(text))\n",
    "            if(len(text) > 0):\n",
    "                rect_xywh_txt.append([x,y,w,h])\n",
    "        \n",
    "        if len(rect_xywh_txt) == 0:\n",
    "            return\n",
    "        \n",
    "        rect_xywh_sorted = sorted(rect_xywh_txt, key = lambda t:t[1])\n",
    "        tolerance_y = int(self.ing_promo.shape[0]*0.06)\n",
    "        rect_xywh_promos = []\n",
    "        rect_xywh_promos.append([rect_xywh_sorted[0]])\n",
    "        current_miny_rect = rect_xywh_sorted[0]\n",
    "        idx = 0\n",
    "        for rect in rect_xywh_sorted[1:]:\n",
    "            if rect[1] < current_miny_rect[1] + tolerance_y:\n",
    "                rect_xywh_promos[idx].append(rect)\n",
    "            else:\n",
    "                idx = idx+1\n",
    "                rect_xywh_promos.append([rect])\n",
    "                current_miny_rect = rect\n",
    "                \n",
    "        rect_xywh_promo_combined = []\n",
    "        \n",
    "        for rects in rect_xywh_promos:\n",
    "            rects_arr = np.array(rects)\n",
    "            n_rows = rects_arr.shape[0]\n",
    "            right_arr = (rects_arr[:, 0] + rects_arr[:, 2]).reshape(n_rows, 1)\n",
    "            bottom_arr= (rects_arr[:, 1] + rects_arr[:, 3]).reshape(n_rows, 1)\n",
    "            rects_arr= np.hstack((rects_arr, right_arr, bottom_arr))\n",
    "            \n",
    "            min_arr = np.min(rects_arr, axis = 0)\n",
    "            max_arr = np.max(rects_arr, axis=0)\n",
    "            min_x, min_y = min_arr[0], min_arr[1]\n",
    "            max_right, max_bottom = max_arr[4], max_arr[5]\n",
    "            \n",
    "            rect_xywh_promo_combined.append([min_x, min_y, max_right, max_bottom])\n",
    "\n",
    "        rowsCT = self.imgCropTest.shape[0]\n",
    "        colsCT = self.imgCropTest.shape[1]\n",
    "        if (rowsCT >= 1500) or (colsCT >= 1500):\n",
    "            thickness = 10\n",
    "        elif (rowsCT >= 500) or (colsCT >= 500):\n",
    "            thickness = 4 + (colsCT/1000 - 0.5)*6\n",
    "        else:\n",
    "            thickness = 4\n",
    "            \n",
    "        for xywh in rect_xywh_promo_combined:\n",
    "            cv.rectangle(self.imgCropTest, (int(xywh[0]), int(xywh[1])), (int(xywh[2]), int(xywh[3])), (0,255,240), thickness=thickness)\n",
    "\n",
    "        self.noOfPromo.append(len(rect_xywh_promo_combined))\n",
    "\n",
    "        return\n",
    "    \n",
    "    def _classify_boxes(self, bound_box, rack_info):\n",
    "        ind = rack_info[0]\n",
    "        labInd = rack_info[1]\n",
    "                \n",
    "        rowsCT = self.imgCropTest.shape[0]\n",
    "        colsCT = self.imgCropTest.shape[1]\n",
    "\n",
    "        self.ing_promo = self.ing.copy()\n",
    "        \n",
    "        # print(rowsCT, colsCT)\n",
    "        x_max = np.amax(bound_box[:,3])\n",
    "        x_min = np.amin(bound_box[bound_box[:,1] > 0,1])\n",
    "\n",
    "        shelfRatios = np.zeros((len(np.unique(labInd)),16))\n",
    "        shelfCoverage = np.zeros((len(np.unique(labInd)),16))\n",
    "        shelfStats = [None]*16\n",
    "        boxClass = [None]*16\n",
    "        length = np.zeros((len(np.unique(labInd)),1))\n",
    "        infoList = []\n",
    "        # include = [False]*16\n",
    "\n",
    "        # for visibility index\n",
    "        finalCoverData = []; placements = []; coverage = []; xInfo = [None]*16; vacInfo = [];\n",
    "\n",
    "        size = np.ones((16,1))\n",
    "        # print('Starting classification module...')\n",
    "        noOfShelves = len(np.unique(labInd))\n",
    "        for i in range(noOfShelves):\n",
    "            index = np.where(labInd == i)\n",
    "            bbox = bound_box[ind[index],:]\n",
    "\n",
    "            # To remove zero size boxes\n",
    "            if len(np.unique(bbox[:,0])) == 1:\n",
    "                continue\n",
    "            infoList.append(i)\n",
    "            \n",
    "            rowCoverDataA = []\n",
    "            rowCoverDataB = []\n",
    "            rowUncoverData = []\n",
    "            rowPlacement = []\n",
    "\n",
    "            # To remove boxes at the edges of the images\n",
    "            bbox = np.sort(bbox, axis = 0)\n",
    "            if 0 in np.unique(bbox):\n",
    "                locs = np.unique(np.where(bbox == 0)[0])\n",
    "                # print(locs)\n",
    "                if len(np.unique(bbox[locs,:])) == 1:\n",
    "                    bbox = bbox[np.unique(np.where(bbox != 0)[0]),:]\n",
    "                else:\n",
    "                    for loc in locs:\n",
    "                        if len(np.where(bbox[loc, :] == 0)[0]) == 4:\n",
    "                            bbox[loc,:] = np.nan\n",
    "                    bbox = bbox[~np.any(np.isnan(bbox), axis=1)]\n",
    "\n",
    "            maxX = 0\n",
    "            minX = np.amin(bbox[:,1])\n",
    "            images = np.zeros((bbox.shape[0],224,224,3)) \n",
    "            for j in range(bbox.shape[0]):\n",
    "                testImg = self.imgCropTest[int(rowsCT*bbox[j,0]):int(rowsCT*bbox[j,2])+1,int(colsCT*bbox[j,1]):int(colsCT*bbox[j,3])+1]\n",
    "                testImgR = cv.resize(testImg,(224,224))\n",
    "                images[j,:,:,:] = testImgR\n",
    "            pred_label = self.loaded_model.predict(images/255)\n",
    "\n",
    "            if bbox[:,[1,3]].shape[0] < 3:\n",
    "                cluster_info = [0]*(bbox[:,[1,3]].shape[0])\n",
    "            else:\n",
    "                cluster_info = KMeans(n_clusters = 3).fit_predict(bbox[:,[1,3]])\n",
    "            \n",
    "            lastLabel = 0\n",
    "            newClusterLabels = []\n",
    "            for cluster in range(len(cluster_info)):\n",
    "                if cluster == 0:\n",
    "                    newClusterLabels.append(lastLabel)\n",
    "                else:\n",
    "                    if cluster_info[cluster - 1] != cluster_info[cluster]:\n",
    "                        lastLabel += 1\n",
    "                    newClusterLabels.append(lastLabel)\n",
    "            cluster_info = newClusterLabels\n",
    "\n",
    "            # print(\"Cluster Info: \", cluster_info)\n",
    "\n",
    "            y_min = int(np.min(bbox[:,0]*rowsCT))\n",
    "            y_max = int(np.max(bbox[:,2]*rowsCT))\n",
    "            self.ing_promo[y_min:y_max, :] = 0\n",
    "            # print(pred_label, np.sum(pred_label, axis=1))\n",
    "\n",
    "            #print(np.max(pred_label,1),np.argmax(pred_label,1))\n",
    "            for j in range(bbox.shape[0]):\n",
    "                # if np.max(pred_label[j]) < 0.3:\n",
    "                #     continue\n",
    "                if shelfStats[np.argmax(pred_label[j,:])] == None:\n",
    "                    shelfStats[np.argmax(pred_label[j,:])] = []\n",
    "                    boxClass[np.argmax(pred_label[j,:])] = []\n",
    "                    xInfo[np.argmax(pred_label[j,:])] = [np.zeros((noOfShelves, 1)), np.zeros((noOfShelves, 1)), np.zeros((noOfShelves, 1))]\n",
    "                    # include[np.argmax(pred_label[j,:])] = True\n",
    "\n",
    "                shelfStats[np.argmax(pred_label[j,:])].append(bbox[j,3]-np.maximum(bbox[j,1],maxX))\n",
    "                boxClass[np.argmax(pred_label[j,:])].append([i,j])\n",
    "                shelfRatios[i,np.argmax(pred_label[j,:])] += 1\n",
    "                shelfCoverage[i,np.argmax(pred_label[j,:])] += bbox[j,3]-np.maximum(bbox[j,1],maxX)\n",
    "\n",
    "                rowPlacement.append(np.argmax(pred_label[j,:]))\n",
    "                rowCoverDataA.append(bbox[j,3])\n",
    "                rowCoverDataB.append(np.maximum(bbox[j,1], maxX))\n",
    "\n",
    "                xInfo[np.argmax(pred_label[j,:])][cluster_info[j]][i] += 1\n",
    "                \n",
    "                maxX = bbox[j,3]\n",
    "                size[np.argmax(pred_label[j,:])] = np.minimum(size[np.argmax(pred_label[j,:])],bbox[j,3]-bbox[j,1])\n",
    "\n",
    "                if j == 0:\n",
    "                    # print(\"xmin\", infoList.index(i), bbox[j, 1], x_min)\n",
    "                    vacInfo.append([infoList.index(i), bbox[j, 1] - x_min, -1, np.argmax(pred_label[j+1,:])])\n",
    "                    # print(\"x\", infoList.index(i), bbox[j+1, 1], bbox[j, 3])\n",
    "                    vacInfo.append([infoList.index(i), bbox[j+1, 1] - bbox[j, 3], -1, np.argmax(pred_label[j+1,:])])\n",
    "                elif j == bbox.shape[0]-1:\n",
    "                    blankSpace = x_max - bbox[j, 3]\n",
    "                    # print(\"xmax\", infoList.index(i), x_max, bbox[j, 3])\n",
    "                    vacInfo.append([infoList.index(i), x_max - bbox[j, 3], np.argmax(pred_label[j-1,:]), -1])\n",
    "                else:\n",
    "                    # print(\"x\", infoList.index(i), bbox[j+1, 1], bbox[j, 3])\n",
    "                    vacInfo.append([infoList.index(i), bbox[j+1, 1] - bbox[j, 3], np.argmax(pred_label[j-1,:]), np.argmax(pred_label[j+1,:])])\n",
    "\n",
    "                # if bbox[j,1] > maxX:\n",
    "                #     rowUncoverData.append()\n",
    "\n",
    "            placements.append(rowPlacement)\n",
    "            coverage.append([rowCoverDataA, rowCoverDataB])\n",
    "            length[i] = 0.12*(x_max - x_min) + 0.88*(maxX - minX)\n",
    "            # print(\"i \", length)\n",
    "\n",
    "        self._draw_promo_boundary()\n",
    "\n",
    "        # print('Shelf coverage calculated...')\n",
    "        # print(\"All Shelves: \", xInfo)\n",
    "        \n",
    "        shelfRatios = shelfRatios[infoList,:]\n",
    "        shelfCoverage = shelfCoverage[infoList,:]\n",
    "        length = length[infoList,:]\n",
    "        for i in range(16):\n",
    "            if xInfo[i] != None:\n",
    "                xInfo[i][0] = xInfo[i][0][infoList]\n",
    "                xInfo[i][1] = xInfo[i][1][infoList]\n",
    "                xInfo[i][2] = xInfo[i][2][infoList]\n",
    "        # print(\"Selected Shelves: \", xInfo)\n",
    "\n",
    "        weights = self._generate_weights(len(placements))\n",
    "        heatBucket = np.zeros((17, 1))\n",
    "        for i in range(len(weights)):\n",
    "            S = 100 * weights[i] / np.sum(weights)\n",
    "            for j in range(len(placements[i])):\n",
    "                x2 = 1000*coverage[i][1][j]\n",
    "                x1 = 1000*coverage[i][0][j]\n",
    "                n = (2*length[i][0] - x1 - x2) * (x2 - x1)\n",
    "                d = np.square(1000*length[i][0])\n",
    "                heatBucket[placements[i][j]] += S*n/d\n",
    "        # print(heatBucket, np.sum(heatBucket))\n",
    "        # heatBucket = heatBucket / np.sum(heatBucket)\n",
    "        heatBucket[16] = np.sum(heatBucket)\n",
    "        heatBucket = heatBucket[heatBucket != 0]\n",
    "        heatBucket[heatBucket < 0] = 0\n",
    "        if heatBucket[-1:] > 100:\n",
    "            heatBucket[-1:] = 100\n",
    "            heatBucket[:-1] = 100*heatBucket[:-1]/np.sum(heatBucket[:-1])\n",
    "        # print(\"HB:    \",heatBucket)\n",
    "\n",
    "        # toReturn = self._process_for_data(shelfRatios, shelfCoverage, shelfStats, length, size, heatBucket, xInfo)\n",
    "        toReturn = self._process_for_data(shelfRatios, shelfCoverage, shelfStats, length, vacInfo, heatBucket, xInfo, infoList)\n",
    "        # for i in range(16):\n",
    "        #     try:\n",
    "        #         if toReturn[2][i] != None:\n",
    "        #             toReturn[2][i][0] = toReturn[2][i][0][infoList]\n",
    "        #             toReturn[2][i][1] = toReturn[2][i][1][infoList]\n",
    "        #             toReturn[2][i][2] = toReturn[2][i][2][infoList]\n",
    "        #     except ValueError as ex:\n",
    "        #         pass\n",
    "\n",
    "        locs = toReturn[4]\n",
    "        for i in range(len(locs)):\n",
    "            # print(locs[i])\n",
    "            boxLoc = boxClass[locs[i][0]][locs[i][1]]\n",
    "            # print(boxLoc)\n",
    "            index = np.where(labInd == boxLoc[0])\n",
    "            bbox = bound_box[ind[index],:]\n",
    "            bbox = np.sort(bbox, axis = 0)\n",
    "            finalBox = bbox[boxLoc[1],:]\n",
    "            # print(finalBox)\n",
    "            x = finalBox[1] * colsCT\n",
    "            y = finalBox[0] * rowsCT\n",
    "            right = finalBox[3] * colsCT\n",
    "            bottom = finalBox[2] * rowsCT\n",
    "            # print(int(x), int(y), int(right), int(bottom))\n",
    "            cv.rectangle(self.ing, (int(x), int(y)), (int(right), int(bottom)), (255,255,255), thickness=8)\n",
    "\n",
    "        return toReturn[0], toReturn[1], toReturn[2], toReturn[3], toReturn[5] \n",
    "\n",
    "    def _generate_weights(self, len):\n",
    "        \n",
    "        if len == 2:\n",
    "            return [1, 1]\n",
    "        elif len == 3:\n",
    "            return [1,3,2]\n",
    "        elif len == 4:\n",
    "            return [2, 2.5, 2.5, 1]\n",
    "        elif len == 5:\n",
    "            return [1, 3, 5, 4, 2]\n",
    "\n",
    "        return []\n",
    "\n",
    "    def _max_find(self, one, two):\n",
    "        if one < 3:\n",
    "            maxOne = 0\n",
    "        elif one <= 5:\n",
    "            maxOne = 5\n",
    "        elif one < 10:\n",
    "            maxOne = 10\n",
    "        else:\n",
    "            maxOne = 15\n",
    "        \n",
    "        if two < 3:\n",
    "            maxTwo = 0\n",
    "        elif two <= 5:\n",
    "            maxTwo = 5\n",
    "        elif one < 10:\n",
    "            maxTwo = 10\n",
    "        else:\n",
    "            maxTwo = 15\n",
    "\n",
    "        return maxOne, maxTwo\n",
    "\n",
    "    def _contest_compliance(self, one, two):\n",
    "        maxOne, maxTwo = self._max_find(one, two)\n",
    "        # print(maxOne, maxTwo)\n",
    "        if maxOne == 0:\n",
    "            return 2\n",
    "        elif maxTwo == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            if ((one+1)/maxOne > 1) and ((two+1)/maxTwo > 1):\n",
    "                return 0\n",
    "            if ((one+1)/maxOne < 1) and ((two+1)/maxTwo < 1):\n",
    "                if (one+1)/maxOne == (two+1)/maxTwo:\n",
    "                    return 0\n",
    "                elif (one+1)/maxOne > (two+1)/maxTwo:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 2\n",
    "            else:\n",
    "                if ((one+1)/maxOne > 1):\n",
    "                    return 2\n",
    "                else:\n",
    "                    return 1\n",
    "\n",
    "    # def _process_for_data(self, shelfRatios, shelfCoverage, shelfStats, length, size, heatBucket, xInfo):\n",
    "    def _process_for_data(self, shelfRatios, shelfCoverage, shelfStats, length, vacInfo, heatBucket, xInfo, infoList):\n",
    "\n",
    "        # prodNames = ['Kent','Chesterfield','2000','Muratti','Monte_Carlo','Pall_Mall','LD','LM','Camel','Marlboro','Parliament','Lark','Lucky_Strike','Davidoff','Viceroy','Winston','West']\n",
    "        prodNames = ['Kent','Chesterfield','2000','Muratti','MonteCarlo','PallMall','LM','LuckyStrike','Marlboro','Parliament','Lark','Other','Davidoff','Viceroy','Winston','West']\n",
    "\n",
    "        shlfDf = pd.DataFrame(np.transpose(shelfRatios))\n",
    "        shlfDf.set_index([prodNames], inplace = True)\n",
    "        shlfDf.columns = ['Shelf_'+ str(i + 1) for i in range(shlfDf.shape[1])]\n",
    "        shlfDf['Total'] = shlfDf.sum(axis = 1)\n",
    "        shlfDf.Total.replace(0, np.nan, inplace = True)\n",
    "        shlfDf.dropna(subset = ['Total'], axis = 0, inplace = True)\n",
    "        shlfDf['Number_of_Products_(%)'] = 100*shlfDf.Total/shlfDf.Total.sum()\n",
    "\n",
    "        df = pd.DataFrame(shelfCoverage)\n",
    "        df.columns = prodNames\n",
    "        df['Length Of Rack'] = length\n",
    "        df = df.T\n",
    "        df.columns = ['Shelf_' + str(i+1) for i in range(df.shape[1])]\n",
    "        df['Occupied_Product_Area_(%)'] = df.sum(axis=1)\n",
    "        df['Occupied_Product_Area_(%)'].replace(0,np.nan,inplace=True)\n",
    "        df.dropna(subset=['Occupied_Product_Area_(%)'],axis=0,inplace=True)\n",
    "\n",
    "        #statsDf = pd.DataFrame(index = [prodNames], columns = [\"Mean\", \"Std. Dev\", \"Median\", \"Q1\", \"Q3\"])\n",
    "        #for j in range(len(shelfStats)):\n",
    "           # prodStat = shelfStats[j]\n",
    "            #if prodStat == None:\n",
    "                #continue\n",
    "            #statsDf.at[prodNames[j], \"Mean\"] = np.mean(prodStat)\n",
    "            #statsDf.at[prodNames[j], \"Std. Dev\"] = np.std(prodStat)\n",
    "            #statsDf.at[prodNames[j], \"Median\"] = np.median(prodStat)\n",
    "            #statsDf.at[prodNames[j], \"Q1\"] = np.percentile(prodStat,25)\n",
    "            #statsDf.at[prodNames[j], \"Q3\"] = np.percentile(prodStat,75)\n",
    "        #statsDf.dropna(subset=['Mean'],axis=0,inplace=True)\n",
    "\n",
    "        statsDf = pd.read_fwf('/data1/naquib.alam/stats/ShelfStatDF_354.txt')\n",
    "        statsDf.set_index([statsDf['Unnamed: 0'].tolist()], inplace = True)\n",
    "        statsDf.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "        statsDf = statsDf.loc[df.index.tolist()[:-1]]\n",
    "\n",
    "        vacancyComp = [None]*16\n",
    "        # print(vacInfo)\n",
    "        for i in range(len(vacInfo)):            \n",
    "            stn = str(int(vacInfo[i][0]) + 1)\n",
    "            # print(i)\n",
    "            if vacInfo[i][2] == vacInfo[i][3]:\n",
    "                width = statsDf['Mean'][prodNames[vacInfo[i][2]]] - 1.08*statsDf['Std. Dev'][prodNames[vacInfo[i][2]]]\n",
    "                numberOfProdsPossible = np.floor(vacInfo[i][1]/width)\n",
    "                # print(\"If Out: \", i, vacInfo[i][0], vacInfo[i][1], vacInfo[i][2], vacInfo[i][3], width, numberOfProdsPossible)\n",
    "                if numberOfProdsPossible <= 0:\n",
    "                    continue\n",
    "                # print(vacancyComp[vacInfo[i][2]])\n",
    "                try:\n",
    "                    if vacancyComp[vacInfo[i][2]] == None:\n",
    "                        vacancyComp[vacInfo[i][2]] = np.zeros((len(infoList), 1))\n",
    "                except ValueError as ex:\n",
    "                    pass\n",
    "                \n",
    "                vacancyComp[vacInfo[i][2]][vacInfo[i][0]] += numberOfProdsPossible\n",
    "            else:\n",
    "                # print(\"Else Out: \", vacInfo[i])\n",
    "                # print('Shelf_'+stn, prodNames[vacInfo[i][2]])\n",
    "                if (vacInfo[i][2] == -1) or (vacInfo[i][3] == -1):\n",
    "                    if vacInfo[i][2] == -1:\n",
    "                        prodOneCount = 0\n",
    "                        prodTwoCount = shlfDf['Shelf_'+stn][prodNames[vacInfo[i][3]]]\n",
    "                        width = [10, statsDf['Mean'][prodNames[vacInfo[i][3]]]]\n",
    "                    else:\n",
    "                        prodOneCount = shlfDf['Shelf_'+stn][prodNames[vacInfo[i][2]]]\n",
    "                        prodTwoCount = 0\n",
    "                        width = [statsDf['Mean'][prodNames[vacInfo[i][2]]], 10]\n",
    "                else:\n",
    "                    prodOneCount = shlfDf['Shelf_'+stn][prodNames[vacInfo[i][2]]]\n",
    "                    prodTwoCount = shlfDf['Shelf_'+stn][prodNames[vacInfo[i][3]]]\n",
    "                    width = [statsDf['Mean'][prodNames[vacInfo[i][2]]] - statsDf['Std. Dev'][prodNames[vacInfo[i][2]]], statsDf['Mean'][prodNames[vacInfo[i][3]]] - statsDf['Std. Dev'][prodNames[vacInfo[i][3]]]]\n",
    "\n",
    "                # print(i, width, prodOneCount, prodTwoCount)\n",
    "                \n",
    "                blankSpace = vacInfo[i][1]\n",
    "                flag = False\n",
    "\n",
    "                try:\n",
    "                    # print(\"Blankspace: \", blankSpace, width[0], width[1])\n",
    "                    if (blankSpace < width[0]) and (blankSpace < width[1]):\n",
    "                        raise Exception\n",
    "\n",
    "                    while blankSpace > 0:\n",
    "                        # print(\"Enter while for different boxes\")\n",
    "                        try:\n",
    "                            # print(\"-_-\")\n",
    "                            if flag == False:\n",
    "                                chosenProd = self._contest_compliance(prodOneCount, prodTwoCount)\n",
    "                            # print(\"chosenIn: \",chosenProd)\n",
    "                            \n",
    "                            if chosenProd == 0:\n",
    "                                chosenProd = 1\n",
    "\n",
    "                            numberOfProdsPossible = np.floor(blankSpace/width[chosenProd-1])\n",
    "                            if numberOfProdsPossible < 0:\n",
    "                                numberOfProdsPossible = 0\n",
    "                            # print(\"In: \", i, vacInfo[i][0], blankSpace, vacInfo[i][2], vacInfo[i][3], chosenProd, numberOfProdsPossible)\n",
    "\n",
    "                            if numberOfProdsPossible == 0:\n",
    "                                chosenProd = -chosenProd + 3\n",
    "                                if flag == False:\n",
    "                                    flag = True\n",
    "                                    raise GetOutOfLoop\n",
    "                                else:\n",
    "                                    raise Exception \n",
    "                            if vacancyComp[vacInfo[i][1+chosenProd]] == None:\n",
    "                                vacancyComp[vacInfo[i][1+chosenProd]] = np.zeros((df.shape[1], 1))\n",
    "\n",
    "                            vacancyComp[vacInfo[i][1+chosenProd]][vacInfo[i][0]] += 1\n",
    "                            blankSpace -= width[chosenProd-1]\n",
    "                            if chosenProd == 1:\n",
    "                                prodOneCount += 1\n",
    "                            else:\n",
    "                                prodTwoCount += 1\n",
    "\n",
    "                        except GetOutOfLoop:\n",
    "                            pass\n",
    "\n",
    "                except Exception:\n",
    "                    pass\n",
    "        # print(vacancyComp)\n",
    "        \n",
    "        vacancyDf = pd.DataFrame([], ['Shelf_'+ str(i + 1) for i in range(shlfDf.shape[1]-2)], [])\n",
    "        count = 0\n",
    "        for j in range(len(vacancyComp)):\n",
    "            try:\n",
    "                if vacancyComp[j] == None:\n",
    "                    count += 1\n",
    "                    continue\n",
    "            except ValueError as ex:\n",
    "                vacancyDf[prodNames[j]] = vacancyComp[j]#[infoList]\n",
    "                pass\n",
    "        # print(vacancyDf)\n",
    "        if count == 16:\n",
    "            vacancyDf[shlfDf.index.tolist()[0]] = np.zeros((shlfDf.shape[1]-2,1))\n",
    "\n",
    "\n",
    "        quantFail = [None]*statsDf.shape[0]\n",
    "        lenFail = [None]*statsDf.shape[0]\n",
    "        locs = []\n",
    "\n",
    "        horzDf = pd.DataFrame([], [\"Left\", \"Middle\", \"Right\"], [])\n",
    "        for j in range(len(prodNames)):\n",
    "            if xInfo[j] != None:\n",
    "                horzDf[prodNames[j]] = xInfo[j]\n",
    "        # print(horzDf)\n",
    "\n",
    "        for j in range(statsDf.shape[0]):\n",
    "            prodStat = shelfStats[prodNames.index(statsDf.index.tolist()[j])]\n",
    "            #print(\"Product: \", statsDf.index.tolist()[j])\n",
    "            #print(\"Data from Image: \", prodStat)\n",
    "            #print(\"Overall Data: \", statsDf.iloc[j,:])\n",
    "            iqf = statsDf.iloc[j, 1] #- statsDf.iloc[j, 3]\n",
    "            #print('IQF: ', iqf)\n",
    "            for i in range(len(prodStat)):\n",
    "                if (prodStat[i] < statsDf.iloc[j,0] - 3*iqf) or (prodStat[i] > statsDf.iloc[j,0] + 3*iqf):\n",
    "                    locs.append([prodNames.index(statsDf.index.tolist()[j]),i])\n",
    "                    \n",
    "            pQLess = sum(a < statsDf.iloc[j,0] - 3*iqf for a in prodStat)\n",
    "            pQMore = sum(a > statsDf.iloc[j,0] + 3*iqf for a in prodStat)\n",
    "            #print(pQLess, pQMore)\n",
    "            quantFail[j] = pQLess + pQMore\n",
    "            \n",
    "            pSLess = sum(a for a in prodStat if a < statsDf.iloc[j,0] - 3*iqf)\n",
    "            pSMore = sum(a for a in prodStat if a > statsDf.iloc[j,0] + 3*iqf)\n",
    "            lenFail[j] = pSLess + pSMore\n",
    "\n",
    "        quantFail = [a for a in quantFail if a != None]\n",
    "        quantFail.append(shlfDf.Total.sum())\n",
    "        lenFail = [a for a in lenFail if a != None]\n",
    "        lenFail.append(df.iloc[df.shape[0]-1, df.shape[1]-1])\n",
    "        \n",
    "        statsDf = pd.DataFrame([quantFail, lenFail])\n",
    "        columns = shlfDf.index.tolist()\n",
    "        columns.append(\"Total\")\n",
    "        statsDf.columns = columns\n",
    "        statsDf = statsDf.T\n",
    "        statsDf.columns = [\"Shelf_1\", \"Shelf_2\"]\n",
    "        statsDf[\"Heat\"] = heatBucket\n",
    "        \n",
    "        # size = size[size != 1]\n",
    "\n",
    "        # return shlfDf, df, size, statsDf, locs, horzDf\n",
    "        return shlfDf, df, vacancyDf, statsDf, locs, horzDf\n",
    "    \n",
    "    def _write_results(self, results, labInd, inputfolder):\n",
    "        # result = Image.fromarray(self.ing)\n",
    "        # if self.ing.shape[0] > 1200:\n",
    "            # rows = 1200\n",
    "            # cols = int(1200*self.ing.shape[1]/self.ing.shape[0])\n",
    "        # else:\n",
    "        rows = self.ing.shape[0]\n",
    "        cols = self.ing.shape[1]\n",
    "\n",
    "        if rows > 1200:\n",
    "            aspectRatio = rows/cols\n",
    "            cols = 1200\n",
    "            rows = int(cols/aspectRatio)\n",
    "\n",
    "\n",
    "        fullpath = os.path.join('/data1/naquib.alam/static/results', inputfolder)\n",
    "\n",
    "        # resultC = Image.fromarray(cv.resize(self.ing,(rows,cols)))\n",
    "        r = re.compile(r'result_\\d+.jpg$')\n",
    "        l = []\n",
    "        for f in os.listdir(fullpath):\n",
    "            if not os.path.isfile(os.path.join(fullpath, f)): continue;\n",
    "            # print(f)\n",
    "            if os.path.isfile(os.path.join(fullpath, f)) & (f[0:7]=='result_'): \n",
    "                l = f;\n",
    "                # print('So So True ', True)\n",
    "\n",
    "        if len(l) == 0:\n",
    "            end = 1\n",
    "        else:\n",
    "            end = int(l[7:-4])+1\n",
    "        # print(end)\n",
    "\n",
    "        font = cv.FONT_HERSHEY_SIMPLEX\n",
    "        txt = \"\"\n",
    "        listOfIndices = results[0].index.tolist()\n",
    "        rowsCT = self.imgCropTest.shape[0]\n",
    "        colsCT = self.imgCropTest.shape[1]\n",
    "        if (rowsCT >= 1500) or (colsCT >= 1500):\n",
    "            thickness = 10\n",
    "            inc = 70\n",
    "        elif (rowsCT >= 500) or (colsCT >= 500):\n",
    "            thickness = 5 + (colsCT/1000 - 0.5)*5\n",
    "            inc = 20 + (colsCT/1000 - 0.5)*50\n",
    "        else:\n",
    "            thickness = 5\n",
    "            inc = 20\n",
    "\n",
    "        y = rowsCT - 50\n",
    "        for i in range(results[0].shape[0]):\n",
    "            txt = \"{}: {}\".format(listOfIndices[i], results[0]['Total'][i])\n",
    "            cv.putText(self.ing, txt, (50, y), font, 2, (255,255,255), 4)\n",
    "            y -= inc\n",
    "        txt = \"{}: {}\".format('Total', results[0]['Total'].sum())\n",
    "        cv.putText(self.ing, txt, (50, y), font, 2, (255,255,255), 4)\n",
    "            \n",
    "        \n",
    "        cv.imwrite(fullpath + '/result_' + str(end) + '.jpg', cv.resize(self.ing, (rows, cols)))\n",
    "        cv.imwrite(fullpath + '/resultP_' + str(end) + '.jpg', cv.resize(self.imgCropTest, (rows, cols)))\n",
    "        # cv.imwrite('/data1/naquib.alam/static/resultC_'+str(end)+'.jpg', cv.resize(self.ing,(rows,cols)))\n",
    "        # result.save('/data1/naquib.alam/static/result_'+str(end)+'.jpg')\n",
    "        # resultC.save('/data1/naquib.alam/static/resultC_'+str(end)+'.jpg')\n",
    "        # print('Resulting image saved...')\n",
    "        \n",
    "        df = results[1]\n",
    "        # size = results[2]\n",
    "        vacancy = results[2]\n",
    "\n",
    "        with open(fullpath + '/ProdCount_' + str(end) + '.txt', 'w') as f:\n",
    "           f.write(round(results[0]).to_string())\n",
    "        with open(fullpath + '/ProdHorz_' + str(end) + '.txt', 'w') as f:\n",
    "           f.write(round(results[4].T).to_string())\n",
    "        with open(fullpath + '/ProdShare_' + str(end) + '.txt', 'w') as f:\n",
    "           occupy = df[\"Occupied_Product_Area_(%)\"]\n",
    "           occupy = occupy[-1:]\n",
    "           # print(occupy.to_string()) \n",
    "           resultDf = round(100*df/df.iloc[df.shape[0]-1])\n",
    "           resultDf = resultDf.iloc[[i for i in range(resultDf.shape[0]-1)]]\n",
    "           resultDf[\"Length\"] = occupy[0]\n",
    "           f.write(resultDf.to_string())\n",
    "\n",
    "        # print(vacancy)\n",
    "\n",
    "        if vacancy.T.shape[0] != 0:\n",
    "            with open(fullpath + '/ProdVacancy_' + str(end) + '.txt', 'w') as f:\n",
    "                # vacancy = np.zeros((resultDf.shape[0], resultDf.shape[1]-2))\n",
    "\n",
    "                # lengthOfRacks = df.iloc[df.shape[0]-1]\n",
    "                # for i in range(df.shape[1]-1):\n",
    "                #     st = 'Shelf_' + str(i+1)\n",
    "                #     if resultDf[st].sum() < 99.9:\n",
    "                #         space = (100 - resultDf[st].sum())*lengthOfRacks[st]/100\n",
    "                #         vacancy[:,i] = space/size\n",
    "                # vacancy[(vacancy < 1) & (vacancy > 0.95)] = 1\n",
    "                # vacancy = np.floor(vacancy)\n",
    "\n",
    "                # vacDf = pd.DataFrame(vacancy)\n",
    "                # vacDf.set_index(results[0].index,inplace=True)\n",
    "\n",
    "                # vacDf.columns = ['Shelf_' + str(i+1) for i in range(df.shape[1]-1)]\n",
    "\n",
    "                f.write(vacancy.T.to_string())\n",
    "               \n",
    "               #f.write('Rackwise Shelf Share:\\n')\n",
    "               # for i in range(len(np.unique(labInd))):\n",
    "               #     val = 100*df.loc[i].values/df.loc[i,'sum']\n",
    "               #     f.write('Rack ' + str(i) + '\\n' + str(val[:-1]) + '\\n')\n",
    "               #     f.write(str(np.sum(val[:-1])) + '\\n')\n",
    "               # val = 100*df.loc[len(np.unique(labInd))].values/df.loc[len(np.unique(labInd)),'sum']\n",
    "               # f.write('\\nOverall Shelf Share: \\n'+ str(val[:-1]) + '\\n')\n",
    "               # f.write(str(np.sum(val[:-1])))\n",
    "\n",
    "        with open(fullpath + '/ProdSkew_' + str(end) + '.txt', 'w') as f:\n",
    "           f.write(round(results[3], 2).to_string())\n",
    "\n",
    "        # print('Shelf-Share calculated and saved...\\n')\n",
    "        \n",
    "    def main(self, inputfile, inputfolder):\n",
    "        # self.trial()\n",
    "        self._image_load(inputfile)\n",
    "        self._sess = tf.Session()\n",
    "        self._sess.graph.as_default()\n",
    "        tf.import_graph_def(self.graph_def, name='')\n",
    "        \n",
    "        self._find_object()\n",
    "        dataTable = self._process_boxes(inputfolder)\n",
    "        return dataTable\n",
    "        \n",
    "def main_fn(inputfolder):\n",
    "    shelf = ShelfShare()\n",
    "    # for f in os.listdir('static'):\n",
    "    #     if os.path.isfile(os.path.join('static', f)):\n",
    "    #         os.remove(os.path.join('static',f))\n",
    "\n",
    "    fullpath = os.path.join('/data1/naquib.alam/static/results', inputfolder)\n",
    "    try:\n",
    "        os.mkdir(fullpath)\n",
    "    except OSError as exc:\n",
    "        try:\n",
    "            os.mkdir('/data1/naquib.alam/static/results')\n",
    "            os.mkdir(fullpath)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "            pass\n",
    "\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise\n",
    "        pass\n",
    "\n",
    "    path = os.path.join('/data1/naquib.alam/static/temp', inputfolder)\n",
    "    noOfShelves = 0\n",
    "    for f in os.listdir(path):\n",
    "        if os.path.isfile(os.path.join(path, f)):\n",
    "            if f[0:7] == \"result_\":\n",
    "                table = shelf.main(os.path.join(path, f), inputfolder)\n",
    "                noOfShelves += 1\n",
    "\n",
    "    with open(fullpath + '/timeStamp.txt', 'w') as f:\n",
    "        f.write(str(datetime.datetime.fromtimestamp(time.time()).strftime('%d-%m-%Y %H:%M:%S')) + \"\\n\"+inputfolder + \"\\nABC\\n\" + str(noOfShelves) + \"\\nPromo \" + str(shelf.noOfPromo))\n",
    "\n",
    "    return table\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main_fn(os.path.join('/data1/naquib.alam/static/temp', inputfolder))\n",
    "\n",
    "#main_fn(sys.argv[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these generated metrics are shown as follows:\n",
    "1. __SKU count__  \n",
    "  ![title](images/Product_count_rsa.png)\n",
    "  \n",
    "  \n",
    "2. __SKU shelf share__\n",
    "  ![title](images/Product_shelf_share_rsa.png)\n",
    "  \n",
    "  \n",
    "3. __SKU vacancy__  \n",
    "A few important points regarding the calculation of this metric are as follows:\n",
    "    1. Average width of each SKU from the whole training image is found.\n",
    "    2. Each vacancy is checked for if an SKU can be placed there or not.\n",
    "    3. Priorities for an SKU to be placed in any of these vacancies are as follows:\n",
    "        1. Adjacent SKUs\n",
    "        2. An SKU from upper rack\n",
    "        3. An SKU from below rack\n",
    "        4. Any other SKU based on predicted count\n",
    " \n",
    "   ![title](images/Product_vacancy_rsa.png)\n",
    "  \n",
    "  \n",
    "  \n",
    "4. __Out of stock SKUs__  \n",
    "A few important points regarding the calculation of this metric are as follows:\n",
    "  \n",
    "    1. In order to find out of stock SKUs, users are asked for the shelf IDs.\n",
    "    2. Compliance table for these shelf IDs in the database are checked.\n",
    "    3. Predicted counts and values from database are compared for each SKU on that shelf.\n",
    "    4. Different alarms are raised depending on how low the predicted count is from actual count\n",
    "        1. __Low__: if between 75% and 50%\n",
    "        2. __Medium__: if between 50% and 25%\n",
    "        3. __High__: if between 25% and 0%\n",
    "        4. __Critical__: if there are zero products\n",
    "   ![title](images/Out_of_stock_rsa.png)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promo Detection Module\n",
    "A few implementation details regarding this module are as follows:\n",
    "1. Since this dataset didn't have annotations for displayed promotions, we couldn't use deep learning based techniques.\n",
    "2. All the regions which have been predicted as presence of an SKU were filled with zero (black pixels).\n",
    "3. All the rack dividers were filled with zeros too.\n",
    "4. Entire image were __thresholded__ and then __Contour Detection__ were used.\n",
    "5. False positives were filtered using contour areas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/Promo_detection_rsa.png)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
